import { generateText } from 'ai';
import { ollama } from 'ollama-ai-provider-v2';

export class AISDKService {
  private model: string;
  private static instance: AISDKService | null = null;

  private constructor() {
    this.model =
      (typeof process !== 'undefined' && process.env?.OLLAMA_MODEL) ||
      (typeof process !== 'undefined' &&
        process.env?.NEXT_PUBLIC_OLLAMA_MODEL) ||
      'llama3.2:3b';

    console.log(`ü§ñ AISDKService initialized with model: ${this.model}`);
  }

  public static getInstance(): AISDKService {
    if (!AISDKService.instance) {
      AISDKService.instance = new AISDKService();
    }
    return AISDKService.instance;
  }

  /**
   * Generate text using AI SDK with Ollama
   */
  async generateText(params: {
    prompt: string;
    system?: string;
    maxTokens?: number;
  }): Promise<string> {
    const { prompt, system, maxTokens = 4000 } = params;

    console.log(`üöÄ Starting AI SDK generation:`);
    console.log(`   üìã Model: ${this.model}`);
    console.log(`   üìù Prompt length: ${prompt.length} chars`);
    console.log(`   ‚öôÔ∏è  System prompt: ${system ? 'Yes' : 'No'}`);
    console.log(`   üéØ Max tokens: ${maxTokens}`);

    try {
      console.log(`üì§ Sending request to Ollama via AI SDK...`);

      const startTime = Date.now();

      const { text } = await generateText({
        model: ollama(this.model),
        prompt,
        system,
      });

      const endTime = Date.now();
      const duration = endTime - startTime;

      console.log(`‚úÖ AI SDK response received:`);
      console.log(`   ‚è±Ô∏è  Duration: ${duration}ms`);
      console.log(`   üìä Response length: ${text?.length || 0} chars`);

      if (!text || text.trim() === '') {
        console.warn(`‚ö†Ô∏è  AI SDK returned an empty response`);
        return 'Analysis failed';
      }

      console.log(`‚úÖ AI SDK generation succeeded!`);
      return text;
    } catch (error) {
      console.error(`‚ùå AI SDK generation failed:`);
      console.error(`   üîç Error:`, error);

      throw new Error(
        `Local LLM not available. Install Ollama: https://ollama.ai and pull model: ollama pull ${this.model}`,
      );
    }
  }

  /**
   * Generate response with system prompt (compatibility method)
   */
  async generateResponse(
    prompt: string,
    systemPrompt?: string,
  ): Promise<string> {
    return this.generateText({
      prompt,
      system: systemPrompt,
    });
  }

  /**
   * ChatGPT-compatible method
   */
  async chatCompletion(
    messages: Array<{ role: string; content: string }>,
  ): Promise<string> {
    console.log(`üí¨ Starting ChatGPT-compatible conversation:`);
    console.log(`   üí≠ Messages: ${messages.length}`);

    const systemMessages = messages.filter((m) => m.role === 'system');
    const userMessages = messages.filter((m) => m.role === 'user');
    const assistantMessages = messages.filter((m) => m.role === 'assistant');

    console.log(`   ‚öôÔ∏è  System messages: ${systemMessages.length}`);
    console.log(`   üë§ User messages: ${userMessages.length}`);
    console.log(`   ü§ñ Assistant messages: ${assistantMessages.length}`);

    const systemMessage =
      messages.find((m) => m.role === 'system')?.content || '';
    const lastUserMessage =
      userMessages[userMessages.length - 1]?.content || '';

    return this.generateResponse(lastUserMessage, systemMessage);
  }

  /**
   * Analyze reviews (compatibility method)
   */
  async analyzeReviews(reviewTexts: string): Promise<string> {
    console.log(`üìä Starting review analysis:`);
    console.log(`   üìù Reviews: ${reviewTexts.split('\n').length}`);
    console.log(`   üìè Total length: ${reviewTexts.length} chars`);

    const systemPrompt = `You are a customer review analyst. Provide concise and helpful responses in English.`;

    const prompt = `Analyze the following customer reviews and report:
1. Most common positive themes
2. Most common negative themes
3. Suggestions for improvement
4. Summary of customer experiences

Reviews:
${reviewTexts}`;

    return this.generateResponse(prompt, systemPrompt);
  }
}
