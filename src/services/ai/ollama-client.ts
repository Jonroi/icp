import axios from 'axios';

export class OllamaClient {
  private ollamaUrl: string;
  private model: string;

  constructor(model: string = 'llama3.2:3b-instruct-q4_K_M') {
    this.ollamaUrl = 'http://localhost:11434/api/generate';
    this.model = model;
    console.log(`ğŸ¤– OllamaClient initialized with model: ${this.model}`);
  }

  async generateResponse(
    prompt: string,
    systemPrompt?: string,
  ): Promise<string> {
    console.log(`ğŸš€ Starting LLM generation:`);
    console.log(`   ğŸ“‹ Model: ${this.model}`);
    console.log(`   ğŸ”— URL: ${this.ollamaUrl}`);
    console.log(`   ğŸ“ Prompt length: ${prompt.length} chars`);
    console.log(`   âš™ï¸  System prompt: ${systemPrompt ? 'Yes' : 'No'}`);

    try {
      const fullPrompt = systemPrompt
        ? `<|system|>${systemPrompt}</s><|user|>${prompt}</s><|assistant|>`
        : `<|user|>${prompt}</s><|assistant|>`;

      console.log(`ğŸ“¤ Sending request to Ollama...`);

      const startTime = Date.now();
      const response = await axios.post(this.ollamaUrl, {
        model: this.model,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          max_tokens: 2048,
        },
      });

      const endTime = Date.now();
      const duration = endTime - startTime;

      console.log(`âœ… LLM response received:`);
      console.log(`   â±ï¸  Duration: ${duration}ms`);
      console.log(
        `   ğŸ“Š Response length: ${response.data.response?.length || 0} chars`,
      );
      console.log(`   ğŸ¯ Status: ${response.status}`);

      const result = response.data.response || 'Analysis failed';

      if (result === 'Analysis failed') {
        console.warn(`âš ï¸  LLM returned an empty response`);
      } else {
        console.log(`âœ… LLM generation succeeded!`);
      }

      return result;
    } catch (error) {
      console.error(`âŒ LLM generation failed:`);
      console.error(`   ğŸ” Error:`, error);

      if (axios.isAxiosError(error)) {
        console.error(`   ğŸ“¡ HTTP Status: ${error.response?.status}`);
        console.error(`   ğŸ“„ Response:`, error.response?.data);
      }

      throw new Error(
        `Local LLM not available. Install Ollama: https://ollama.ai and pull model: ollama pull ${this.model}`,
      );
    }
  }

  async analyzeReviews(reviewTexts: string): Promise<string> {
    console.log(`ğŸ“Š Starting review analysis:`);
    console.log(`   ğŸ“ Reviews: ${reviewTexts.split('\n').length}`);
    console.log(`   ğŸ“ Total length: ${reviewTexts.length} chars`);

    const systemPrompt = `You are a customer review analyst. Provide concise and helpful responses in English.`;

    const prompt = `Analyze the following customer reviews and report:
1. Most common positive themes
2. Most common negative themes
3. Suggestions for improvement
4. Summary of customer experiences

Reviews:
${reviewTexts}`;

    return this.generateResponse(prompt, systemPrompt);
  }

  // ChatGPT-compatible method
  async chatCompletion(
    messages: Array<{ role: string; content: string }>,
  ): Promise<string> {
    console.log(`ğŸ’¬ Starting ChatGPT-compatible conversation:`);
    console.log(`   ğŸ’­ Messages: ${messages.length}`);

    const systemMessages = messages.filter((m) => m.role === 'system');
    const userMessages = messages.filter((m) => m.role === 'user');
    const assistantMessages = messages.filter((m) => m.role === 'assistant');

    console.log(`   âš™ï¸  System messages: ${systemMessages.length}`);
    console.log(`   ğŸ‘¤ User messages: ${userMessages.length}`);
    console.log(`   ğŸ¤– Assistant messages: ${assistantMessages.length}`);

    const systemMessage =
      messages.find((m) => m.role === 'system')?.content || '';
    const lastUserMessage =
      userMessages[userMessages.length - 1]?.content || '';

    return this.generateResponse(lastUserMessage, systemMessage);
  }
}
