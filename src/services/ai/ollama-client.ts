import axios from 'axios';

export class OllamaClient {
  private ollamaUrl: string;
  private model: string;

  constructor(model: string = 'llama3.2:3b-instruct-q4_K_M') {
    this.ollamaUrl = 'http://localhost:11434/api/generate';
    this.model = model;
    console.log(`ğŸ¤– OllamaClient alustettu mallilla: ${this.model}`);
  }

  async generateResponse(
    prompt: string,
    systemPrompt?: string,
  ): Promise<string> {
    console.log(`ğŸš€ Aloitetaan LLM-generaatio:`);
    console.log(`   ğŸ“‹ Malli: ${this.model}`);
    console.log(`   ğŸ”— URL: ${this.ollamaUrl}`);
    console.log(`   ğŸ“ Prompt pituus: ${prompt.length} merkkiÃ¤`);
    console.log(`   âš™ï¸  System prompt: ${systemPrompt ? 'KyllÃ¤' : 'Ei'}`);

    try {
      const fullPrompt = systemPrompt
        ? `<|system|>${systemPrompt}</s><|user|>${prompt}</s><|assistant|>`
        : `<|user|>${prompt}</s><|assistant|>`;

      console.log(`ğŸ“¤ LÃ¤hetetÃ¤Ã¤n pyyntÃ¶ Ollamalle...`);

      const startTime = Date.now();
      const response = await axios.post(this.ollamaUrl, {
        model: this.model,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          max_tokens: 2048,
        },
      });

      const endTime = Date.now();
      const duration = endTime - startTime;

      console.log(`âœ… LLM-vastaus saatu:`);
      console.log(`   â±ï¸  Kesto: ${duration}ms`);
      console.log(
        `   ğŸ“Š Vastauksen pituus: ${
          response.data.response?.length || 0
        } merkkiÃ¤`,
      );
      console.log(`   ğŸ¯ Status: ${response.status}`);

      const result = response.data.response || 'Analyysi epÃ¤onnistui';

      if (result === 'Analyysi epÃ¤onnistui') {
        console.warn(`âš ï¸  LLM palautti tyhjÃ¤n vastauksen`);
      } else {
        console.log(`âœ… LLM-generaatio onnistui!`);
      }

      return result;
    } catch (error) {
      console.error(`âŒ LLM-generaatio epÃ¤onnistui:`);
      console.error(`   ğŸ” Virhe:`, error);

      if (axios.isAxiosError(error)) {
        console.error(`   ğŸ“¡ HTTP Status: ${error.response?.status}`);
        console.error(`   ğŸ“„ Response:`, error.response?.data);
      }

      throw new Error(
        `Paikallinen LLM ei ole saatavilla. Asenna Ollama: https://ollama.ai ja lataa malli: ollama pull ${this.model}`,
      );
    }
  }

  async analyzeReviews(reviewTexts: string): Promise<string> {
    console.log(`ğŸ“Š Aloitetaan arvostelujen analyysi:`);
    console.log(`   ğŸ“ Arvosteluja: ${reviewTexts.split('\n').length} kpl`);
    console.log(`   ğŸ“ Kokonaispituus: ${reviewTexts.length} merkkiÃ¤`);

    const systemPrompt = `Olet asiakasarvostelujen analysoija. Anna ytimekkÃ¤itÃ¤ ja hyÃ¶dyllisiÃ¤ vastauksia suomeksi.`;

    const prompt = `Analysoi seuraavat asiakasarvostelut ja kerro:
1. YleisimmÃ¤t positiiviset teemat
2. YleisimmÃ¤t negatiiviset teemat  
3. Parannusehdotukset
4. Yhteenveto asiakkaiden kokemuksista

Arvostelut:
${reviewTexts}`;

    return this.generateResponse(prompt, systemPrompt);
  }

  // ChatGPT-kompatiibeli metodi
  async chatCompletion(
    messages: Array<{ role: string; content: string }>,
  ): Promise<string> {
    console.log(`ğŸ’¬ Aloitetaan ChatGPT-kompatiibeli keskustelu:`);
    console.log(`   ğŸ’­ Viestien mÃ¤Ã¤rÃ¤: ${messages.length}`);

    const systemMessages = messages.filter((m) => m.role === 'system');
    const userMessages = messages.filter((m) => m.role === 'user');
    const assistantMessages = messages.filter((m) => m.role === 'assistant');

    console.log(`   âš™ï¸  System viestejÃ¤: ${systemMessages.length}`);
    console.log(`   ğŸ‘¤ User viestejÃ¤: ${userMessages.length}`);
    console.log(`   ğŸ¤– Assistant viestejÃ¤: ${assistantMessages.length}`);

    const systemMessage =
      messages.find((m) => m.role === 'system')?.content || '';
    const lastUserMessage =
      userMessages[userMessages.length - 1]?.content || '';

    return this.generateResponse(lastUserMessage, systemMessage);
  }
}
